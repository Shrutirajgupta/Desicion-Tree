{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOrkQrLwz+H4YYWk1Oex2Sw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**Theory** **Questions**"],"metadata":{"id":"eD2_reqJ674W"}},{"cell_type":"markdown","source":["1. What is a Decision Tree, and how does it work?\n","\n",">A Decision Tree is a supervised learning algorithm used for classification and regression tasks. It works by recursively splitting the data into subsets based on feature values, forming a tree structure where each internal node represents a decision rule, and each leaf node represents a class label (classification) or a numerical value (regression).\n","\n","\n","2. What are impurity measures in Decision Trees?\n","\n",">Impurity measures quantify the heterogeneity of a dataset. Common impurity measures include:\n","\n",">Gini Impurity: Measures how often a randomly chosen element would be incorrectly classified.\n","\n",">\n","Entropy: Measures the amount of disorder in the dataset.\n","\n","\n","\n","3. What is the mathematical formula for Gini Impurity?\n","\n",">Gini = 1 - \\sum_{i=1}^{C} p_i^2\n","\n","\n","\n","4. What is the mathematical formula for Entropy?\n","\n",">Entropy = - \\sum_{i=1}^{C} p_i \\log_2(p_i)\n","\n","\n","5. What is Information Gain, and how is it used in Decision Trees?\n","\n",">Information Gain (IG) measures the reduction in entropy after a dataset is split based on a feature.\n","\n","IG = Entropy(Parent) - \\sum_{i=1}^{k} \\frac{|S_i|}{|S|} Entropy(S_i)\n","\n","\n","\n","6. What is the difference between Gini Impurity and Entropy?\n","\n",">Gini is computationally faster since it doesn't require logarithms.\n","\n",">Entropy considers the probabilistic nature of uncertainty and can provide more interpretable results.\n","\n","\n","\n","7. What is the mathematical explanation behind Decision Trees?\n","\n",">Decision Trees recursively split the dataset using criteria like Gini or Entropy. The optimal split is determined by maximizing Information Gain or minimizing impurity. Recursive splitting stops based on stopping criteria like maximum depth, minimum samples per leaf, or pure nodes.\n","\n","\n","8. What is Pre-Pruning in Decision Trees?\n","\n",">Pre-Pruning stops tree growth early by setting constraints like max_depth, min_samples_split, or min_samples_leaf.\n","\n","\n","9. What is Post-Pruning in Decision Trees?\n","\n",">Post-Pruning grows a full tree first and then removes branches that do not improve accuracy using techniques like Cost Complexity Pruning (CCP).\n","\n","\n","10. What is the difference between Pre-Pruning and Post-Pruning?\n","\n",">Pre-Pruning prevents overfitting early but might underfit.\n","\n",">Post-Pruning builds a full tree first and then simplifies it.\n","\n","\n","\n","11. What is a Decision Tree Regressor?\n","\n",">A Decision Tree Regressor predicts continuous values by averaging the target variable in leaf nodes.\n","\n","\n","12. What are the advantages and disadvantages of Decision Trees?\n","\n","->Advantages:\n","\n","*Easy to interpret\n","\n","*Handles both numerical and categorical data\n","\n","*Requires little data preprocessing\n","\n","->Disadvantages:\n","\n","*Prone to overfitting\n","\n","*Sensitive to small data changes\n","\n","*Can be biased if dataset is imbalanced\n","\n","\n","\n","13. How does a Decision Tree handle missing values?\n","\n",">Uses surrogate splits: If a primary feature is missing, an alternative feature is used.\n","\n",">Uses mean/mode imputation.\n","\n","\n","\n","14. How does a Decision Tree handle categorical features?\n","\n",">Uses one-hot encoding or label encoding.\n","\n","Splits based on unique category values.\n","\n","\n","\n","15. What are some real-world applications of Decision Trees?\n","\n",">Credit risk assessment\n","\n",">Medical diagnosis\n","\n",">Fraud detection\n","\n",">Customer segmentation\n","\n",">SpamÂ filtering\n","\n"],"metadata":{"id":"8I0Ky11p7J0j"}},{"cell_type":"markdown","source":["#**Practical** **Questions**"],"metadata":{"id":"IgoC78369JwY"}},{"cell_type":"markdown","source":["16.  Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy."],"metadata":{"id":"LIbMOkt39ZYy"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IOQfM8Hc6slt","executionInfo":{"status":"ok","timestamp":1743404187284,"user_tz":-330,"elapsed":11911,"user":{"displayName":"Shruti Gupta","userId":"00406294573104776226"}},"outputId":"ac16f919-abed-43d3-98b9-f6fc5974d9e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 1.0\n"]}],"source":["from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","iris = load_iris()\n","X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n","\n","model = DecisionTreeClassifier()\n","model.fit(X_train, y_train)\n","y_pred = model.predict(X_test)\n","\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"]},{"cell_type":"markdown","source":["17. Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the\n","feature importances."],"metadata":{"id":"GGnP96kd9fl2"}},{"cell_type":"code","source":["model = DecisionTreeClassifier(criterion=\"gini\")\n","model.fit(X_train, y_train)\n","\n","print(\"Feature Importances:\", model.feature_importances_)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IC1wPDdG9pn2","executionInfo":{"status":"ok","timestamp":1743404187351,"user_tz":-330,"elapsed":62,"user":{"displayName":"Shruti Gupta","userId":"00406294573104776226"}},"outputId":"6cd54c88-de9e-4d9e-fd2e-00d1be85f7d7"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Feature Importances: [0.03334028 0.         0.88947325 0.07718647]\n"]}]},{"cell_type":"markdown","source":["18. Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the\n","model accuracy."],"metadata":{"id":"Z791TXqk9qdc"}},{"cell_type":"code","source":["model = DecisionTreeClassifier(criterion=\"entropy\")\n","model.fit(X_train, y_train)\n","y_pred = model.predict(X_test)\n","\n","from sklearn.metrics import mean_squared_error\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KGfRrIg59uMt","executionInfo":{"status":"ok","timestamp":1743404760709,"user_tz":-330,"elapsed":23,"user":{"displayName":"Shruti Gupta","userId":"00406294573104776226"}},"outputId":"7333b4ca-098c-4f9d-9738-9bfc6387f643"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 1.0\n"]}]},{"cell_type":"markdown","source":["19. Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean\n","Squared Error.\n"],"metadata":{"id":"uVyWwH2k9u65"}},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeRegressor\n","from sklearn.metrics import mean_squared_error\n","from sklearn.datasets import fetch_california_housing\n","\n","housing = fetch_california_housing()\n","X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.2, random_state=42)\n","\n","model = DecisionTreeRegressor()\n","model.fit(X_train, y_train)\n","y_pred = model.predict(X_test)\n","\n","print(\"MSE:\", mean_squared_error(y_test, y_pred))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7LmoarHv90JA","executionInfo":{"status":"ok","timestamp":1743404224810,"user_tz":-330,"elapsed":2063,"user":{"displayName":"Shruti Gupta","userId":"00406294573104776226"}},"outputId":"f06c0c5d-5fea-4593-f3b4-8f5bc9c71294"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["MSE: 0.5010536440253875\n"]}]},{"cell_type":"markdown","source":["20. Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz."],"metadata":{"id":"az7yXldp90oE"}},{"cell_type":"code","source":["from sklearn.tree import export_graphviz\n","import graphviz\n","\n","dot_data = export_graphviz(model, out_file=None, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\n","graph = graphviz.Source(dot_data)\n","graph.view()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"8hHJ5jUh94rr","executionInfo":{"status":"ok","timestamp":1743404755799,"user_tz":-330,"elapsed":146,"user":{"displayName":"Shruti Gupta","userId":"00406294573104776226"}},"outputId":"73c8ddaa-6eb6-4059-f133-5c1d97a1e683"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Source.gv.pdf'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["21. Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its\n","accuracy with a fully grown tree."],"metadata":{"id":"IytJy8hj95XS"}},{"cell_type":"code","source":["model_restricted = DecisionTreeClassifier(max_depth=3)\n","model_restricted.fit(X_train, y_train)\n","print(\"Restricted Accuracy:\", accuracy_score(y_test, model_restricted.predict(X_test)))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jgdVkNwj98tz","executionInfo":{"status":"ok","timestamp":1743404752350,"user_tz":-330,"elapsed":22,"user":{"displayName":"Shruti Gupta","userId":"00406294573104776226"}},"outputId":"6c4037f5-e71c-499e-b057-b08c6e3cff0c"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Restricted Accuracy: 1.0\n"]}]},{"cell_type":"markdown","source":["22. Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its\n","accuracy with a default tree."],"metadata":{"id":"I6q8THyH99g1"}},{"cell_type":"code","source":["model = DecisionTreeClassifier(min_samples_split=5)\n","model.fit(X_train, y_train)\n","print(\"Accuracy:\", accuracy_score(y_test, model.predict(X_test)))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KiYxuCfS-Blz","executionInfo":{"status":"ok","timestamp":1743404748472,"user_tz":-330,"elapsed":28,"user":{"displayName":"Shruti Gupta","userId":"00406294573104776226"}},"outputId":"ae578447-1463-4a2b-88d9-4e4b0727e02b"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 1.0\n"]}]},{"cell_type":"markdown","source":["23. Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its\n","accuracy with unscaled data."],"metadata":{"id":"D79AQ2gR-Crg"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","model = DecisionTreeClassifier()\n","model.fit(X_train_scaled, y_train)\n","print(\"Accuracy with Scaling:\", accuracy_score(y_test, model.predict(X_test_scaled)))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-maIBley-GOm","executionInfo":{"status":"ok","timestamp":1743404744330,"user_tz":-330,"elapsed":57,"user":{"displayName":"Shruti Gupta","userId":"00406294573104776226"}},"outputId":"ab700ba9-3687-4aee-fdd8-e63f077b78ce"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy with Scaling: 1.0\n"]}]},{"cell_type":"markdown","source":["24. Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass\n","classification."],"metadata":{"id":"hG_McaSY-HQP"}},{"cell_type":"code","source":["from sklearn.multiclass import OneVsRestClassifier\n","\n","model = OneVsRestClassifier(DecisionTreeClassifier())\n","model.fit(X_train, y_train)\n","print(\"OvR Accuracy:\", accuracy_score(y_test, model.predict(X_test)))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u1XuHuMz-OFZ","executionInfo":{"status":"ok","timestamp":1743404740086,"user_tz":-330,"elapsed":32,"user":{"displayName":"Shruti Gupta","userId":"00406294573104776226"}},"outputId":"be0cedef-d1ab-424d-8ea3-fcd1e8176e06"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["OvR Accuracy: 1.0\n"]}]},{"cell_type":"markdown","source":["25. Write a Python program to train a Decision Tree Classifier and display the feature importance scores."],"metadata":{"id":"2K1CLxCw-OtM"}},{"cell_type":"code","source":["print(\"Feature Importances:\", model.feature_importances_)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m3bxvN1n-ShP","executionInfo":{"status":"ok","timestamp":1743404735424,"user_tz":-330,"elapsed":19,"user":{"displayName":"Shruti Gupta","userId":"00406294573104776226"}},"outputId":"d48765ae-5f0d-4283-a643-39cd1a9e6f4c"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Feature Importances: [0.         0.00851154 0.95207811 0.03941035]\n"]}]},{"cell_type":"markdown","source":["26. Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance\n","with an unrestricted tree."],"metadata":{"id":"nNLR-qdS-UWI"}},{"cell_type":"code","source":["model = DecisionTreeRegressor(max_depth=5)\n","model.fit(X_train, y_train)\n","print(\"MSE:\", mean_squared_error(y_test, model.predict(X_test)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rlzmq-FO-YQg","executionInfo":{"status":"ok","timestamp":1743404728894,"user_tz":-330,"elapsed":24,"user":{"displayName":"Shruti Gupta","userId":"00406294573104776226"}},"outputId":"0a904581-f3ba-45a5-dc13-6940b89007ac"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["MSE: 0.0\n"]}]},{"cell_type":"markdown","source":["27. Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and\n","visualize its effect on accuracy."],"metadata":{"id":"GGreH42N-ZF6"}},{"cell_type":"code","source":["path = model.cost_complexity_pruning_path(X_train, y_train)\n","print(\"CCP Alphas:\", path.ccp_alphas)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GEHVfXqd-crS","executionInfo":{"status":"ok","timestamp":1743404731555,"user_tz":-330,"elapsed":27,"user":{"displayName":"Shruti Gupta","userId":"00406294573104776226"}},"outputId":"8b8a3bec-a537-4afe-c8c1-c2e41629fda6"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["CCP Alphas: [0.         0.00404762 0.00555556 0.00555556 0.00810811 0.0120598\n"," 0.12163269 0.49170139]\n"]}]},{"cell_type":"markdown","source":["28. Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision,\n","Recall, and F1-Score.\n"],"metadata":{"id":"ljhTgaSd-dS6"}},{"cell_type":"code","source":["from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","\n","# Load dataset\n","iris = load_iris()\n","X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n","\n","# Train Decision Tree Classifier\n","model = DecisionTreeClassifier()\n","model.fit(X_train, y_train)\n","\n","# Predictions\n","y_pred = model.predict(X_test)\n","\n","# Evaluate using Precision, Recall, and F1-Score\n","precision = precision_score(y_test, y_pred, average='weighted')\n","recall = recall_score(y_test, y_pred, average='weighted')\n","f1 = f1_score(y_test, y_pred, average='weighted')\n","\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall: {recall:.4f}\")\n","print(f\"F1-Score: {f1:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IB-FWXif-g5y","executionInfo":{"status":"ok","timestamp":1743404723830,"user_tz":-330,"elapsed":53,"user":{"displayName":"Shruti Gupta","userId":"00406294573104776226"}},"outputId":"eb61a030-95bc-4f47-8bdd-0e45168ad171"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Precision: 1.0000\n","Recall: 1.0000\n","F1-Score: 1.0000\n"]}]},{"cell_type":"markdown","source":["29. Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn."],"metadata":{"id":"wHBILw2P-hl7"}},{"cell_type":"code","source":["mport seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","\n","# Compute confusion matrix\n","cm = confusion_matrix(y_test, y_pred)\n","\n","# Plot confusion matrix\n","plt.figure(figsize=(6,5))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=iris.target_names, yticklabels=iris.target_names)\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"Actual\")\n","plt.title(\"Confusion Matrix\")\n","plt.show()\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"rUjzVLDI-lzn","executionInfo":{"status":"error","timestamp":1743404720242,"user_tz":-330,"elapsed":38,"user":{"displayName":"Shruti Gupta","userId":"00406294573104776226"}},"outputId":"cf1bb175-5739-4ab4-8f4c-61e5d59dfb6b"},"execution_count":13,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-13-42ccd2fca960>, line 1)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-42ccd2fca960>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    mport seaborn as sns\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"markdown","source":["30. Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values\n","for max_depth and min_samples_split."],"metadata":{"id":"PHOyWuIE-nIc"}},{"cell_type":"code","source":["rom sklearn.model_selection import GridSearchCV\n","\n","# Define parameter grid\n","param_grid = {\n","    'max_depth': [3, 5, 10, None],\n","    'min_samples_split': [2, 5, 10]\n","}\n","\n","# Perform Grid Search\n","grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5, scoring='accuracy')\n","grid_search.fit(X_train, y_train)\n","\n","# Print best parameters and accuracy\n","print(\"Best Parameters:\", grid_search.best_params_)\n","print(\"Best Accuracy:\", grid_search.best_score_)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"q3hsdKyP-rAK","executionInfo":{"status":"error","timestamp":1743404716336,"user_tz":-330,"elapsed":47,"user":{"displayName":"Shruti Gupta","userId":"00406294573104776226"}},"outputId":"c165eff1-7a04-402d-bf1c-c750bf993403"},"execution_count":12,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-12-7896ad85bb1c>, line 1)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-7896ad85bb1c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    rom sklearn.model_selection import GridSearchCV\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"qCywXMHGBM-X"},"execution_count":null,"outputs":[]}]}